<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="The Narrow Gate: Localized Image-Text Communication in Vision-Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The Narrow Gate: Localized Image-Text Communication in Vision-Language Models</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

  <!-- More Research -->
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <header class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              The Narrow Gate
              <p class="title publication-title">Localized Image-Text Communication in Vision-Language Models</p>
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://alexserra98.github.io/">Alessandro Serra</a><sup>1,2*</sup>,
              </span>
              <span class="author-block">
                <a href="https://francescortu.github.io/">Francesco Ortu</a><sup>1,3*</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/emanuele-panizon-141618153/">Emanuele Panizon</a><sup>1*</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/lucrezia-valeriani-827909194/">Lucrezia Valeriani</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/lorebasile/">Lorenzo Basile</a><sup>1,3</sup>,
              </span>
              <span class="author-block">
                <a href="https://areasciencepark-rit.gitlab.io/lade/alessio.ansuini/">Alessio Ansuini</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/diego-doimo/">Diego Doimo</a><sup>1^</sup>,
              </span>
              <span class="author-block">
                <a href="https://area-rit.gitlab.io/lade/alberto.cazzaniga/">Alberto Cazzaniga</a><sup>1^</sup>
              </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>1</sup>AREA Science Park,</span>
              <span class="author-block"><sup>2</sup>SISSA,</span>
              <span class="author-block"><sup>3</sup>University of Trieste</span>
            </div>
            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup></sup>Trieste, Italy</span>
            </div>

            <div class="is-size-7 publication-authors">
              <span class="author-block"><sup>*</sup>Equal contribution.</span>
              <span class="author-block"><sup>^</sup>Equal supervision.</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/RitAreaSciencePark/Narrow-gate"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/collections/RitAreaSciencePark/narrowgate-67504e977751c5caa4ef72e6"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
              </div>
            </div>
          </header>
        </div>
      </div>
    </div>
  </section>

  <section class="section pt-0">
    <div class="container is-max-desktop">
      <figure>
        <img src="./static/images/cartoon.jpg" class="teaser-image">
      </figure>

  </section>
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width has-text-centered">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              <!-- Recent advances in multimodal training have significantly improved the integration of image understanding and generation within a unified model. 
            This study investigates how vision-language models (VLMs) handle image understanding tasks, specifically focusing on how visual information is processed and transferred to the textual domain. 
            We compare VLMs that generate both images and text with those that output only text, highlighting key differences in information flow.
            We find that in models with multimodal outputs, image and text embeddings are more separated within the residual stream.
            Additionally, models vary in how information is exchanged from visual to textual tokens. VLMs that only output text exhibit a distributed communication pattern, where information is exchanged through multiple image tokens. 
            In contrast, models trained for image and text generation rely on a single token that acts as a narrow gate for the visual information. 
            We demonstrate that ablating this single token significantly deteriorates performance on image understanding tasks. Furthermore, modifying this token enables effective steering of the image semantics, showing that targeted, local interventions can reliably control the model's global behavior.          
          -->
              Recent advances in multimodal training have significantly improved the integration of image understanding
              and generation within a unified model. This study investigates how vision-language models (VLMs) handle
              image understanding tasks, specifically focusing on how visual information is processed and transferred to
              the textual domain. We focus on two architectures: <b>Chameleon</b> [1], which supports multimodal outputs,
              and
              <b>Pixtral</b> [2], which outputs text only. Pixtral fuses late-layer visual tokens into the textual domain,
              as
              it can output only text, and it features a distributed image-text communication pattern, in which internal
              image tokens directly communicate with the textual domain. On the contrary, Chameleon encodes visual and
              textual tokens in well separated regions, and knowledge transfer from image to text happens through a
              <b>narrow gate</b>, the end-of-image token <tt>[EOI]</tt>. We demonstrate that ablating this single token
              significantly deteriorates performance on image understanding tasks. Furthermore, modifying this token
              enables effective steering of the image semantics, showing that targeted, local interventions can reliably
              control the model's global behavior.
            </p>
          </div>
        </div>
      </div>
    </div>
    </div>
    <!--/ Abstract. -->
    </div>
  </section>


  <!-- Predictions. -->
  <section class="section" id="results">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width has-text-centered">

          <h2 class="title is-3">Modality Gap in VLMs</h2>
          <figure>
            <img src="./static/images/figure3.svg" class="teaser-image" style="width:80%" height:auto><br><br>
            <caption><b>Left:</b> Cosine similarity between text and image token embeddings as a function of model depth
              reflects the orthogonality of modalities in Chameleon models.
              <b>Right:</b> Homogeneity score of token clusters generated via Advanced Density Peaks [3] with respect to
              their original modality.
            </caption>
          </figure>

          <div class="content has-text-justified">
            <br>
            <p>
              We highlight a fundamental difference in how multimodal-output and unimodal-output
              VLMs handle the interaction between visual and textual representations. In
              multimodal-output models like Chameleon, image and text representations remain largely separated
              throughout
              the network, as indicated by consistently low cosine similarity and
              modality-specific clustering. This separation raises questions about how and where the modalities
              communicate.
              In contrast, models like Pixtral exhibit progressively higher fusion of image and text
              representations in later layers, as witnessed by rising cosine similarity and decreasing modality
              homogeneity. This
              indicates a more distributed communication pattern, where visual tokens directly inform textual
              generation.
            </p>
          </div>

        </div>
      </div>
    </div>
  </section>

  <section class="section" id="results">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width has-text-centered">

          <h2 class="title is-3">Analysis of Cross-Modal Attention</h2>
          <h2 class="title is-4">Cross-Modal Attention Contributions of Tokens</h2>
          <div class="content has-text-justified">
            <!-- <p> -->
              We construct a metric that quantifies the average attention that all the text tokens give to a
              token at position \(j\) within the image part of the prompt, which we assume to span
              the first \( N_{[\texttt{EOI}]} \) tokens.
              We define the (relative) <b>cross-modal attention</b> \(f^{l}_j\) as:
              \[ f^{l}_{j} = \frac{1}{C}\frac{1}{|\mathcal{H}|}\sum_{h \in \mathcal{H}}
              \sum_{\;\;\;i>N_{[\texttt{EOI}]}} A_{(i,j)}^{l,h} \]
            <!-- </p> -->

            <figure>
              <img src="./static/images/f6_attention_v3.svg" class="teaser-image"><br><br>
              <caption>Contribution of different image token positions to the total text-on-image attention across
                layers
                in Chameleon-7B (<b>left</b>) and Pixtral-12B (<b>right</b>), computed on ImageNet data.
            </figure>
            <p>
              The analysis of total attention contributions reveals that a few localized tokens,
              such as the <tt>[EOI]</tt> and last image tokens in Chameleon-7B, or the <tt>[EOL]</tt>,
              32nd and 1025th image tokens in Pixtral-12B, are strongly attended by text tokens,
              significantly shaping image-to-text communication.
            </p>
            <br>

            <div class="column is-full-width has-text-centered">
            <h2 class="title is-4">Localization of Visual Semantic Information</h2>
            </div>
            To investigate whether these tokens contain semantic visual information, we
            compute their neighborhood overlap with ImageNet class labels defined by [4] as:
            \[ \chi^{l,gt}= \frac{1}{n} \sum_i \frac{1}{k}\sum_{j\in \mathcal{N}^{l}(i)} A^{gt}_{{ij}}\]

            <figure>
              <img src="./static/images/f4_NO.svg" class="teaser-image"><br><br>
              <caption>Neighborhood overlap between selected image tokens and ImageNet labels for Chameleon-7B
                (<b>left</b>) and Pixtral-12B (<b>right</b>).
            </figure>
            <p>
            The results above show that in the Chameleon models the <tt>[EOI]</tt> token is responsible for a
            large portion of cross-modal attention and contains the highest semantic information
            regarding the image. This makes it a suitable candidate for being a <b>narrow gate</b>
            of communication. Conversely, in Pixtral the semantic content is spread on the whole
            image representation, thus suggesting that the model uses a much wider gate of
            communication distributed across large portions of the image.
            </p>

          </div>
        </div>
      </div>
  </section>

  <section class="section" id="results">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width has-text-centered">

          <h2 class="title is-3">Ablation Experiments</h2>
          <h2 class="title is-4">Effect of Progressive Attention Knockout</h2>

          <div class="content has-text-justified">
            We now investigate the impact of special tokens on the flow of information through the network by
            progressively performing attention knockout [5], ablating communication from layer \(l\) to the end of the network. For
            each window of ablated layers, we compute \(\chi^{out,gt}\) to evaluate the effect of the ablation on the
            final representation.

          <figure>
            <img src="./static/images/figure7_v2.svg" class="teaser-image"><br><br>
            <caption>Neighborhood overlap between model final layer rappresentations at the last text token positions
              and ImageNet classes when applying attention knockout.
              Communication from the <tt>[EOI]</tt> token position (<b>green</b>) to the text or from all image token
              positions
              (<b>magenta</b>) to the text is ablated across an increasing number of layers, starting from the last.
          </figure>

          <p>
          These results show that visual information is not directly transmitted between the residual representations of
          image tokens and those of text tokens; instead, it flows through <tt>[EOI]</tt> .
          </p>
          <br>          

          <div class="column is-full-width has-text-centered">
          <h2 class="title is-4">Effect of Attention Knockout on Image Understanding Tasks</h2>
          </div>
          
          We evaluate how applying <b>attention knockout</b> to block communication between specific token positions and
          the text affects the general performance of the models on more complex visual understanding tasks.
         

          <figure>
            <img src="./static/images/table.png" class="teaser-image" style="width:70%" height: auto><br><br>
            <caption>Performance of Chameleon (7B and 34B) and Pixtral models on visual question answering (VQAv2),
              image captioning (Flickr-30k and MS-COCO), and image classification (ImageNet) under different attention
              ablation settings.
          </figure>
          
          <p>
          Overall, the results of our ablation experiments confirm that the cross-modal communication in the Chameleon
          models flows mainly through a single token, the special <tt>[EOI]</tt> token. On the contrary, in Pixtral-12B
          such communication happens in a distributed fashion through the image itself, meaning that it cannot be
          disrupted with a local intervention.
          </p>

        </div>
      </div>
    </div>
  </section>

  <section class="section" id="results">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width has-text-centered">

          <h2 class="title is-3">Steering Image Understanding Through Activation Patching</h2>
          <div class="content has-text-justified">
          The localized nature of cross-modal communication in Chameleon-7B and Chameleon-34B allows a targeted editing
          of image semantics. We show this by performing a patching experiment on the <tt>[EOI]</tt> token, replacing
          that of a base ImageNet class with another target class and measure the similarity between the probability
          distribution of the base class and the target class.


          <figure>
            <img src="./static/images/figure8.svg" class="teaser-image" style="width:80%" height:auto><br><br>
            <caption>Similarity between the probability distributions over the vocabulary for the target input and the
              base input across different layers. <b>Left</b>: Impact of patching the residual stream at each layer.
              <b>Right</b>: Impact of cumulative patching of the sole input of attention blocks, starting from the point
              indicated on the x-axis through the end of the model.
          </figure>

          <p>
          The <tt>[EOI]</tt> token plays a central role in how Chameleon models connect image and text information. Editing
          <tt>[EOI]</tt> in the middle layers effectively transfers semantics between classes, with the strongest impact
          occurring when global image information is encoded. These effects persist until later layers, where the
          semantics have already been communicated to the text, highlighting the token’s critical role in cross-modal
          understanding.
          </p>

        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{,
  title={The Narrow Gate: Localized Image-Text Communication in Vision-Language Models}, 
  author={Alessandro Serra and Francesco Ortu and Emanuele Panizon and Lucrezia Valeriani and Lorenzo Basile and Alessio Ansuini and Diego Doimo and Alberto Cazzaniga},
  year={2024},
  eprint={},
  archivePrefix={},
  primaryClass={},
  url={https://arxiv.org/}, 
}</code></pre>
    </div>
  </section>


  <section class="section" id="ack">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      [1] Chameleon Team. Chameleon: Mixed-modal early-fusion
      foundation models. arXiv preprint arXiv:2405.09818, 2024.
      <br>
      [3] Mistral AI. Pixtral 12B. arXiv preprint arXiv:2410.07073, 2024.
      <br>
      [3] Maria d’Errico, Elena Facco, Alessandro Laio, and Alex
      Rodriguez. Automatic topography of high-dimensional data
      sets by non-parametric density peak clustering. Information Sciences, 2021.
      <br>
      [4] Diego Doimo, Aldo Glielmo, Alessio Ansuini, and Alessandro Laio. Hierarchical nucleation in deep neural networks.
      NeurIPS, 2020.
      <br>
      [5] Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir
      Globerson. Dissecting recall of factual associations in auto-regressive language models. EMNLP, 2023.
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>